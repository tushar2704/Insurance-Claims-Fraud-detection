# -*- coding: utf-8 -*-
"""Insurance Claims Fraud detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMFz1cDrNjAtQ9MEe5XWSIGf4sk4HIpe

# Insurance Claims Fraud detection
### Author github.com/2704

### Importing the required Libraries
"""

import pandas as pd
import pickle
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# Load the dataset from the provided CSV file
df = pd.read_csv('/content/fraud.csv')

# Display information about the dataset
df.info()

# Display the first few rows of the dataset
df.head()

# Replace ' - ' with '_' in the 'PolicyType' column
df['PolicyType'] = df['PolicyType'].str.replace(' - ', '_')

# Replace spaces with underscores throughout the DataFrame
df = df.replace(' ', '_', regex=True)

#Checking for unique values
for column in df:
    # Check if the current column is 'PolicyNumber'
    if column == 'PolicyNumber':
        pass
    else:
        # Print the column name
        print("Column:", column)

        # Print sorted unique values in the column
        unique_values = sorted(df[column].unique())
        print("Unique Values:", unique_values, "\n")

# Extract feature columns by dropping the 'FraudFound_P' column
features = df.drop('FraudFound_P', 1).columns

# Save the list of feature columns to a pickle file
with open("features1.pkl", "wb") as file:
    pickle.dump(features, file)

# Function to extract categorical columns from a DataFrame
def get_categorical_columns(dataframe):
    """
    Extracts and returns a list of categorical column names from the given DataFrame.

    Args:
        dataframe (pd.DataFrame): The DataFrame from which categorical columns are to be extracted.

    Returns:
        list: A list containing the names of categorical columns.
    """
    # Select columns with data type 'object' (categorical)
    categorical_columns = dataframe.select_dtypes(include='object').columns.tolist()
    return categorical_columns

# Call the function to get categorical column names from the DataFrame 'df'
categorical_cols = get_categorical_columns(df)

# Print the list of categorical column names
print("Categorical Columns:", categorical_cols)

# List of categorical columns to be one-hot encoded
columns_to_encode = ['Month', 'DayOfWeek', 'Make', 'AccidentArea', 'DayOfWeekClaimed', 'MonthClaimed', 'Sex', 'MaritalStatus', 'Fault', 'PolicyType', 'VehicleCategory', 'VehiclePrice', 'Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims', 'AgeOfVehicle', 'AgeOfPolicyHolder', 'PoliceReportFiled', 'WitnessPresent', 'AgentType',
                     'NumberOfSuppliments', 'AddressChange_Claim', 'NumberOfCars', 'BasePolicy']

# Fit OneHotEncoder to the specified columns
enc = OneHotEncoder(sparse=False).fit(df.loc[:, columns_to_encode])

# Save the fitted encoder to a pickle file
with open("encoder1.pkl", "wb") as file:
    pickle.dump(enc, file)

from sklearn.model_selection import train_test_split

# Get the names of the encoded columns
column_names = enc.get_feature_names_out(columns_to_encode)

# Encode categorical variables and create a DataFrame
encoded_variables = pd.DataFrame(enc.transform(df.loc[:, columns_to_encode]), columns=column_names)

# Drop original categorical columns from the DataFrame
df = df.drop(columns_to_encode, 1)

# Concatenate the encoded variables DataFrame with the original DataFrame
df = pd.concat([df, encoded_variables], axis=1)

# Separate features (X) and target variable (y)
X, y = df.drop('FraudFound_P', 1), df.loc[:, 'FraudFound_P']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=1)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)

"""## Importing Modeling libraries"""

!pip install -q xgboost lightgbm

import xgboost as xgb
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,

    classification_report,
    precision_recall_curve
)

"""### XGBClassifier"""

# Create an XGBoost model with the binary logistic objective
xgb_model = XGBClassifier(objective='binary:logistic')

# Define a grid of hyperparameters to search through
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.1, 0.01, 0.001],
    'gamma': [0, 0.1, 0.5],
    'subsample': [0.5, 0.75, 1],
    'colsample_bytree': [0.5, 0.75, 1]
}

# Perform Grid Search to find the best combination of hyperparameters
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding accuracy score
print("Best hyperparameters:", grid_search.best_params_)
print("Accuracy score:", grid_search.best_score_)

# Create the best XGBoost model using the best hyperparameters found
best_xgb_model = XGBClassifier(objective='binary:logistic', **grid_search.best_params_)
best_xgb_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_test = best_xgb_model.predict(X_test)

# Generate a classification report to assess the model's performance
report = classification_report(y_test, y_pred_test)
print("Classification Report:\n", report)

"""### LGBMClassifier"""

from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Define a grid of hyperparameters to search through
param_grid = {
    'num_leaves': [10, 20, 30],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
}

# Create a LightGBM model
lgbm_model = LGBMClassifier(objective='binary')

# Perform Grid Search to find the best combination of hyperparameters
grid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Create the best LightGBM model using the best hyperparameters found
best_lgbm_model = LGBMClassifier(**grid_search.best_params_, objective='binary')
best_lgbm_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_test = best_lgbm_model.predict(X_test)

# Generate a classification report to assess the model's performance
report = classification_report(y_test, y_pred_test)
print("Classification Report:\n", report)

"""### Saving the model1"""

# Save the best LightGBM model to a pickle file
pickle.dump(best_lgbm_model, open('model1.pickle', 'wb'))

# Get the list of original column names from the DataFrame
original_columns = df.columns.tolist()

# Print the list of original column names
print("Original Columns:", original_columns)